```{r}
library(rjags)
library(coda)
library(ggplot2)
library(dplyr)
```

## A normal null model

This model is a normal-distribution based random walk model that guesses that the next step of a system is just the previous step plus year-year variability. 

## Developing priors

# A Prior on the observation error

We use the Nelson et al 2015 study that estimated underreporting nationally by about 10-fold using insurance claims. They reported that the CDC numbers (9.4 per 100000) were actually 106.6 per 100000 minus 10% or plus 11.42%. That was based on 2005-2010 data, which includes data from different reporting strategies. 

There are several compounding factors: This is a national estimate of observational error, representing two different CDC reporting strategies, and covering a time period that has little overlap with our model fits. To account for this error, we will say the mean error is a 10-fold increase, but that the varibilty around that error is large- anywhere from a sligth underestimate to a 22-fold increase. This roughly agrees with other liturature mentioned in the paper ( the Hinckley et al paper, which focuses on false negatives from serology tests and the Mead 2015 paper). 

# A prior on yearly variability 
We generated a prior for yearly variability by taking the differences in year-year from non-maine states. We then moment matched that distribution. 

# A prior on X[1]

Our prior on X[1] was based on a poission distribution of the previous-year's count data converted to a rate,  plus yearly observation error. In the case where the previous year was a "<6" character, I treat it as an observed "3". 



```{r}
fit_binomial_null <- function(tick_data, has_suppressed = TRUE, town_name, n.iter = 91000, data_list, init_detection_rate,init_ic ){
  
  tick_data <- tick_disease
  tick_data <- dplyr::arrange(tick_data, by = tick_data$Year) # Make sure to be going in order of year
   
  
  less_than_six_indices <- which(tick_data$Number == "<6" )
  numeric_indices <- which(tick_data$Number != "<6")
  
  data_list$n <- length(tick_data$Year) 
  data_list$numeric_indices <- numeric_indices
  data_list$less_than_six_indices <- less_than_six_indices
  data_list$y <- tick_data$Number
  
  ## Check if has suppressed data. If yes, provide flow control
  if (rlang::is_empty(less_than_six_indices)){
   has_suppressed <- FALSE
  }
  
  if(has_suppressed){
    RandomWalk = "
model{
  
  #### Data Model
  for(t in numeric_indices){
   
   y[t] ~ dgamma(x[t], observation_error) #I think observation error should be 10ish
 }
 
  for(t in less_than_six_indices){   ## Maybe consider a population cutoff, and allow zeros
   rate_cutoff_low <- (1/pop[t]) * 100000 
   rate_cutoff_high <- (5/pop[t]) * 100000 
   y[t] ~ dnorm(x[t], observation_error) T(rate_cutoff_low, rate_cutoff_high)
  }

  #### Process Model
   for (i in 2:n){
   x[i] ~ dnorm(x[i - 1],yearly_variability)
  }

   ### Swap varience for precision
  observation_error <- dnorm(10, 1)
  yearly_variability <- 1/ (prior_on_yearly_variability^2)
  

  #### Priors
  x[1] ~ dnorm(year_before_start_year, fixed_yearly_variability)
  prior_on_yearly_variability ~ dgamma()
}
"
  }else{
        RandomWalk = "
model{
  
  #### Data Model
  for(t in numeric_indices){
   y[t] ~ dnorm(x[t], observation_error)
 }

  #### Process Model
   for (i in 2:n){
   x[i] ~ dnorm(x[i - 1],yearly_variability)
  }

  
  ### Swap varience for precision
  observation_error <- dnorm(10, 1)
  yearly_variability <- 1/ (prior_on_yearly_variability^2)
  

  #### Priors
  x[1] ~ dnorm(year_before_start_year, fixed_yearly_variability)
  prior_on_yearly_variability <- dgamma() 
}
"
  }

## loading in priors
#data <- data_list

## setting  initial conditions
nchain = 3
init <- list()
for(i in 1:nchain){
  init[[i]] <- list( ) # Maybe check  these 
}


j.model   <- jags.model (file = textConnection(RandomWalk),
                             data = data_list,
                             #inits = init,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c( "x", "detection_rate", "probability"),
                                n.iter = n.iter)

GBR <- gelman.plot(jags.out)
burnin <- GBR$last.iter[tail(which(apply(GBR$shrink[,,2]>1.05,1,any)),1)+1] ## Cut our burn-in. 

## Not failing when lack of convergence. Known possible failures to converge: "Pownal", "Frye_Island", "Harrison", "Baldwin"
#Known passes: "Harpswell", "Freeport", "Brunswick", "Yarmouth", "Cumberland", Falmouth",  South_Portland, "Gray", "North_Yarmouth", "Raymond", "Naples", "Casco"
# Rerun for better estimates : Portland "Chebeague_Island" "Standish", "Cape_Elizabeth", "Scarborough","Long_Island", "Gorham" , "New_Gloucester"
if(is.na(burnin)){
  warning("GBR !< 1.05. Model may have failed to converge")
  jags.burn <- jags.out
  did_it_converge <- "convergence_failed_GBR_test" 
  
}else{
  did_it_converge <- "convergence_passed_GBR_test"
  jags.burn <- window(jags.out,start=burnin, extend = FALSE)
}
  date_stamp <- Sys.time()
  date_stamp <- format(date_stamp, "%Y%m%d")
  file_name <- paste("/Users/tess/Documents/work/Maine_Tick_Disease_Forecast/Jags_output/", date_stamp, ".", "binomial_null",".",town_name,".", did_it_converge, ".","JAGS_run.Rdata", sep = "")
  print(did_it_converge)
  print(effectiveSize(jags.burn))
  save(jags.burn, file = file_name )
  return(jags.burn)
}
```


## Looping over every town in cumberland 

note: Frye_Island is failing to converge, becuase a special case of data. Long_Island also failing to converge.  
```{r}
# Special children, failed to converge for maybe structural reasons: c("Pownal", "Harrison", "Baldwin")
## Converged: "Portland",  "Standish", "Cape_Elizabeth", "Scarborough", "Gorham", "New_Gloucester"
## Still more : "Chebeague_Island", "Long_Island" 
#towns_to_run <- c("Chebeague_Island", "Long_Island") # no Frye_Island because NA in data
#"Baldwin" run
# Need to run with larger size "Harrison", "Bridgton"
towns_to_run <- c("Westbrook", "Pownal" ) #Pownal might need longer to converge




for (i in seq_along(towns_to_run)){
  
 town_name <- towns_to_run[i]

tick_data <- tick_disease

  tick_disease_tmp <- tick_data[(tick_data$Location == town_name ),] 
  tick_disease_tmp <- dplyr::filter(tick_disease_tmp, tick_disease_tmp$Year > 2008) # Used 2008 as prior on system
  tick_disease_tmp$Number[tick_disease_tmp$Year > 2016] <- NA # Hold out three years for validation. 
  tick_disease_tmp <- dplyr::arrange(tick_disease_tmp, by = tick_disease_tmp$Year) # Make sure model is fitting through time
  
tick_data <- tick_disease_tmp

data_list <- list(less_than_six_indices = points_of_less_than_six, numeric_indices =numeric_indices, n = 7, a_obs = beta_of_true_tick_rate$alpha, b_obs = beta_of_true_tick_rate$beta , y = tick_disease_tmp$Number,  x_ic = town_x_ic$x_ic[town_x_ic$Location == town_name] , pop = as.numeric(tick_disease_tmp$Population))

print(town_name)
output <- fit_binomial_null(tick_data, has_suppressed = TRUE, town_name, n.iter = 1051000, data_list, init_detection_rate = 0.9, init_ic = as.numeric(tick_data$Number[1]) )
  
}

```

# Lit review for error rates in lyme reporting

*National Estimates of Errors*

Some national error estimates include [this paper](https://academic.oup.com/cid/article/59/5/676/2895755) by Hinckley et al, which estimates the "true" number of infections in 2008 based on the number of specimines sent into labs, and correcting for false positives and false negatives. It does not estimate the number of incidences where the tick itself was not sent in for testing, but says this could add to the number. 

[This CDC summary](https://www.cdc.gov/mmwr/volumes/66/ss/ss6622a1.htm?s_cid=ss6622a1_w) of the 2008-2015 data notes that tick disease rates are underreported in high-incidence areas but possibly over-reported in low-incidence areas.
  Starting in 2008, the CDC explicitly notes the difference between confirmed cases and probable cases. This difference could be used to estimate some of the error, but is also likely an underestimate because the definition of "probable case" still involves some positive lab results/ medical diagnosis. 

[This estimate](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4550147/) of 2005-2010 national incidence rate useing health insurence claims also argues for large numbers of underreporting. 

[This review](https://www-sciencedirect-com.ezproxy.bu.edu/science/article/pii/S0891552015000240?via%3Dihub) as well as being a thourough general review of lyme in the united states, summerizes some of the liturature attemting to estimate the amount of underreporting in lyme disease, putting it aproximatly 3-12 fold differences. The studies were state/ town specific but the techniqes used were: 
 - In Westchestesr New York a "determinisitc model with plausible ranges" [was used](https://academic.oup.com/aje/article/148/10/1018/135572) to estimate "true" incidence levels. 
 - Connecticut Physicians [were surveyed](https://europepmc.org/abstract/med/10186700).
 - Maryland Physicians [were surveyed](https://doi.org/10.1093/infdis/173.5.1260)
 -  Medical records in Wisconsin [were reviewd](https://academic.oup.com/aje/article/155/12/1120/123221)
 
*Maine Estimates of errors*

[There are 2011-2016 estimates of prevelance](https://data.mainepublichealth.gov/tracking/metadata-lyme-prevalence) by town in Maine. Because Prevelance = Incidence x Average duration of disease, we may be able to compare our estimates to prevelance data as measured by the survey. However, prevelance is estimated by the question "Have you ever been told by a doctor, nurse, or other health professional that you have Lyme disease?”. This question does not capture populations that would not have intereacted with medical profesionals. However, it could be a cool benchmark on the state/ on the error assosiated with individuals who do intereact with medical profesionals, but who's disease does not meet the criteria of either probable or confirmed. 

 
